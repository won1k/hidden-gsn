require 'rnn'
require 'hdf5'
require 'nngraph'

cmd = torch.CmdLine()

cmd:option('-rnn_size', 650, 'size of LSTM internal state')
cmd:option('-word_vec_size', 650, 'dimensionality of word embeddings')
cmd:option('-num_layers', 2, 'number of layers in the LSTM')
cmd:option('-epochs', 30, 'number of training epoch')
cmd:option('-learning_rate', 1, 'learning rate')
cmd:option('-start_annealing', 0.2, 'fraction of epochs at which to start annealing learning rate')
cmd:option('-anneal', 0.8, 'multiply learning rate after growing on valid')
cmd:option('-bsize', 32, 'batch size')
cmd:option('-seqlen', 20, 'sequence length')
cmd:option('-max_grad_norm', 5, 'max l2-norm of concatenation of all gradParam tensors')
cmd:option('-auto', 1, '1 if autoencoder (i.e. target = source), 0 otherwise')
cmd:option('-rev', 0, '1 if reversed output, 0 if normal')
cmd:option('-ptb', 1, '1 if ptb')
cmd:option('-adapt', 'none', 'adaptive gradient method (rms/adagrad/adadelta)')
cmd:option('-weight_cost', 0, 'L2 weight decay')
cmd:option('-smooth', 1e8, 'smoothing params')
cmd:option('-dropout_prob', 0.3, 'dropout prob')
cmd:option('-param_init', 0.05, 'initialize paramters')

cmd:option('-data_file','convert/ptb.hdf5','data directory. Should contain data.hdf5 with input data')
cmd:option('-val_data_file','convert/ptb_test.hdf5','data directory. Should contain data.hdf5 with input data')
cmd:option('-gpu', 1, 'which gpu to use. -1 = use CPU')
cmd:option('-savefile', 'checkpoint/ptb-ae','filename to autosave the checkpoint to')
cmd:option('-loadfile', '', 'filename to load encoder/decoder from, if any')

opt = cmd:parse(arg)

END = 3

-- Construct the data set.
local data = torch.class("data")
function data:__init(opt, data_file)
   local f = hdf5.open(data_file, 'r')
   self.input = {}
   self.output = {}
   self.lengths = f:read('sent_lens'):all()
   self.max_len = f:read('max_len'):all()[1]
   self.nfeatures = f:read('nfeatures'):all():long()[1]
   if opt.auto == 1 then
     self.nclasses = f:read('nfeatures'):all():long()[1]
   else
     self.nclasses = f:read('nclasses'):all():long()[1]
   end
   self.length = self.lengths:size(1)
   self.dwin = opt.dwin
   for i = 1, self.length do
     local len = self.lengths[i]
     self.input[len] = f:read(tostring(len)):all():double()
     if opt.auto == 1 then
       self.output[len] = self.input[len]
     else
       self.output[len] = f:read(tostring(len) .. "_target"):all():double()
     end
     if opt.gpu > 0 then
       self.input[len] = self.input[len]:cuda()
       self.output[len] = self.output[len]:cuda()
     end
   end
   f:close()
end

function data:size()
   return self.length
end

function data.__index(self, idx)
   local input, target
   if type(idx) == "string" then
      return data[idx]
   else
      input = self.input[idx]:transpose(1,2)
      output = self.output[idx]:transpose(1,2)
   end
   return {input, output}
end

-- Connect functions for encoder-decoder
function forwardConnect(enc, dec)
   for i = 1, #enc.lstmLayers do
      local seqlen = #enc.lstmLayers[i].outputs
      dec.lstmLayers[i].userPrevOutput = nn.rnn.recursiveCopy(dec.lstmLayers[i].userPrevOutput, enc.lstmLayers[i].outputs[seqlen])
      dec.lstmLayers[i].userPrevCell = nn.rnn.recursiveCopy(dec.lstmLayers[i].userPrevCell, enc.lstmLayers[i].cells[seqlen])
      --dec.lstmLayers[i].userPrevOutput = enc.lstmLayers[i].outputs[seqlen]:clone()
      --dec.lstmLayers[i].userPrevCell = enc.lstmLayers[i].cells[seqlen]:clone()
   end
end

function backwardConnect(enc, dec)
   for i = 1, #enc.lstmLayers do
      enc.lstmLayers[i].userNextGradCell = nn.rnn.recursiveCopy(enc.lstmLayers[i].userNextGradCell, dec.lstmLayers[i].userGradPrevCell)
      enc.lstmLayers[i].gradPrevOutput = nn.rnn.recursiveCopy(enc.lstmLayers[i].gradPrevOutput, dec.lstmLayers[i].userGradPrevOutput)
   end
end

function storeState(dec)
  for i = 1, #dec.lstmLayers do
    dec.lstmLayers[i].userPrevOutput = nn.rnn.recursiveCopy(dec.lstmLayers[i].userPrevOutput, dec.lstmLayers[i].output)
    dec.lstmLayers[i].userPrevCell = nn.rnn.recursiveCopy(dec.lstmLayers[i].userPrevCell, dec.lstmLayers[i].cell)
  end
end


function train(data, valid_data, encoder, decoder, criterion)
   local last_score = 1e9
   -- Set up params
   local encParams, encGradParams = encoder:getParameters()
   local decParams, decGradParams = decoder:getParameters()
   encParams:uniform(-opt.param_init, opt.param_init)
   decParams:uniform(-opt.param_init, opt.param_init)
   local decGradDenom = torch.ones(decGradParams:size())
   local decGradPrevDenom = torch.zeros(decGradParams:size())
   local decPrevGrad = torch.zeros(decGradParams:size())
   local encGradDenom = torch.ones(encGradParams:size())
   local encGradPrevDenom = torch.zeros(encGradParams:size())
   local encPrevGrad = torch.zeros(encGradParams:size())
   if opt.gpu > 0 then
      decGradDenom = decGradDenom:cuda()
      decGradPrevDenom = decGradPrevDenom:cuda()
      decPrevGrad = decPrevGrad:cuda()
      encGradDenom = encGradDenom:cuda()
      encGradPrevDenom = encGradPrevDenom:cuda()
      encPrevGrad = encPrevGrad:cuda()
   end

   -- Set up clones for criterion
   criterionClones = {}
   for t = 1, data.max_len do
       criterionClones[t] = criterion:clone()
   end

   for epoch = 1, opt.epochs do
      print('epoch: ' .. epoch)
      encoder:training()
      decoder:training()
      local trainErr = 0
      local total = 0
      for i = 1, data:size() do
         local sentlen = data.lengths[i]
         print("Sentence length: ", sentlen)
         local d = data[sentlen]
         if opt.ptb > 0 then
          sentlen = sentlen - 1
         end
         local input, output = d[1], d[2]
         local nsent = input:size(2) -- sentlen x nsent input
         --if opt.wide > 0 then
         --  sentlen = sentlen + 2 * torch.floor(data.dwin/2)
         --end
         for sent_idx = 1, torch.ceil(nsent / opt.bsize) do
           local batch_idx = (sent_idx - 1) * opt.bsize
           local batch_size = math.min(sent_idx * opt.bsize, nsent) - batch_idx
           local input_mb = input[{{1, sentlen}, { batch_idx + 1, batch_idx + batch_size }}] -- sentlen x batch_size tensor
           local output_mb = output[{{}, { batch_idx + 1, batch_idx + batch_size }}]
           local revInput
           if opt.rev > 0 then
              revInput = {}
              for t = 1, sentlen do
                table.insert(revInput, input_mb[{{sentlen - t + 1},{}}])
              end
              input_mb = nn.JoinTable(1):forward(revInput)
              if opt.gpu > 0 then
                input_mb = input_mb:cuda()
              end
           end
	   
           -- Encoder forward prop
           local encoderOutput = encoder:forward(input_mb) -- sentlen table of batch_size x rnn_size

           -- Decoder forward prop
           forwardConnect(encoder, decoder)
           local decoderInput = torch.cat(input[{{sentlen + 1}, {batch_idx + 1, batch_idx + batch_size}}], output_mb[{{1, sentlen}, {}}], 1)
           if opt.gpu > 0 then
             decoderInput = decoderInput:cuda()
           else
             decoderInput = decoderInput:double()
           end
           decoderOutput = decoder:forward(decoderInput)

           -- Decoder backward prop
           output_mb = nn.SplitTable(1):forward(output_mb)
	   local gradOutputs = {}
	   for t = 1, sentlen + 1 do
	       trainErr = trainErr + criterionClones[t]:forward(decoderOutput[t], output_mb[t]) * batch_size
	       gradOutputs[t] = criterionClones[t]:backward(decoderOutput[t], output_mb[t])
	       total = total + batch_size
	   end
           decoder:zeroGradParameters()
           decoder:backward(decoderInput, gradOutputs)

           -- Encoder backward prop
           encoder:zeroGradParameters()
           backwardConnect(encoder, decoder)
           local encGrads = {}
           for t = 1, #encoderOutput do
             table.insert(encGrads, encoderOutput[t]:zero())
           end
           encoder:backward(input_mb, encGrads)

           -- Grad norm and update
           local encGradNorm = encGradParams:norm()
           local decGradNorm = decGradParams:norm()
           if encGradNorm > opt.max_grad_norm then
              encGradParams:mul(opt.max_grad_norm / encGradNorm)
           end
           if decGradNorm > opt.max_grad_norm then
              decGradParams:mul(opt.max_grad_norm / decGradNorm)
           end
           --decGradParams, decGradDenom, decGradPrevDenom = adaptiveGradient(
           --    decParams, decGradParams, decGradDenom, decGradPrevDenom, decPrevGrad, opt.adapt)
           --encGradParams, encGradDenom, encGradPrevDenom = adaptiveGradient(
           --    encParams, encGradParams, encGradDenom, encGradPrevDenom, encPrevGrad, opt.adapt)
           -- Parameter update
           --decParams:addcdiv(-opt.learning_rate, decGradParams, decGradDenom)
           --decPrevGrad:mul(0.9):addcdiv(0.1, decGradParams, decGradDenom)
           --encParams:addcdiv(-opt.learning_rate, encGradParams, encGradDenom)
           --encPrevGrad:mul(0.9):addcdiv(0.1, encGradParams, encGradDenom)
           encParams:add(encGradParams:mul(-opt.learning_rate))
           decParams:add(decGradParams:mul(-opt.learning_rate))
           encoder:forget()
           decoder:forget()
        end
      end
      print('Training error', trainErr / total)
      local score, acc = eval(valid_data, encoder, decoder)
      local savefile = string.format('%s_epoch%.2f_%.2f_%.2f',
                                    opt.savefile, epoch, score, acc)
      if epoch % (opt.epochs/5) == 0 then
        torch.save(savefile .. '.t7', {encoder,decoder})
        print('saving checkpoint to ' .. savefile)
      end

      if score > last_score - .3 and epoch > opt.start_annealing * opt.epochs then
         opt.learning_rate = opt.anneal * opt.learning_rate
      end
      last_score = score
      encoder:forget()
      decoder:forget()
      print(epoch, score, acc, opt.learning_rate)
   end
end

function predict(data, encoder, decoder)
   -- Testing
   encoder:evaluate()
   decoder:evaluate()

   -- Test file
   local f = hdf5.open(opt.savefile, "w")
   f:write('sent_lens', data.lengths)   

   -- Setup metrics
   local nll = 0
   local total = 0
   local accuracy = 0

   -- Prediction
   for i = 1, data:size() do
      local sentlen = data.lengths[i]
      local d = data[sentlen]
      local input, output = d[1], d[2]
      if opt.ptb > 0 then
        sentlen = sentlen - 1
      end
      local nsent = input:size(2)
      local revInput
      if opt.rev > 0 then
        revInput = {}
        for t = 1, sentlen do
          table.insert(revInput, input[{{sentlen - t + 1},{}}])
        end
        input = nn.JoinTable(1):forward(revInput)
        if opt.gpu > 0 then
          input = input:cuda()
        end
      end
      output = nn.SplitTable(1):forward(output)

      -- Encoder forward prop
      local encoderOutput = encoder:forward(input[{{1, sentlen}}]) -- sentlen table of batch_size x rnn_size
      -- Decoder forward prop
      forwardConnect(encoder, decoder)
      local decoderInput = { input[{{sentlen + 1}}] }
      decoder:remember()
      local decoderOutput = { decoder:forward(decoderInput[1])[1]:clone() }
      nll = nll + criterion:forward(decoderOutput[1], output[1]) * nsent
      total = total + nsent
      for t = 2, sentlen + 1 do
        local _, nextInput = decoderOutput[t-1]:max(2)
        table.insert(decoderInput, nextInput:reshape(1,nsent):clone())
        table.insert(decoderOutput, decoder:forward(decoderInput[t])[1]:clone())
	nll = criterion:forward(decoderOutput[t], output[t]) * nsent
	total = total + nsent
      end

      -- Save predictions/accuracy (0 if past END)
      local predictions = torch.zeros(nsent, sentlen)
      for  t = 1, #decoderOutput do
      	   local _, pred = decoderOutput[t]:max(2)
	   pred = pred:reshape(nsent)
	   for i = 1, nsent do
	       	if predictions[i][t-1] == 0 or predictions[i][t-1] == END then
	       	    predictions[i][t] = 0
	       	else
		    predictions[i][t] = pred[i]
		end
		if predictions[i][t] == output[t][i] then
		    accuracy = accuracy + 1
		end
	   end		    
      end
      f:write(tostring(sentlen), predictions)

      encoder:forget()
      decoder:forget()
   end

   -- Summary/close
   f:close()
   local valid = math.exp(nll / total)
   print("Test error", valid)
   print("Test accuracy", accuracy / total)
   return valid, accuracy/total
end

function main()
    -- parse input params
   opt = cmd:parse(arg)

   if opt.gpu > 0 then
      print('using CUDA on GPU ' .. opt.gpu .. '...')
      require 'cutorch'
      require 'cunn'
      cutorch.setDevice(opt.gpu)
   end

   -- Create the data loader class.
   local test_data = data.new(opt, opt.data_file)

   -- Load models
   if opt.loadfile:len() > 0 then
      print("Loading old models...")
      local model = torch.load(opt.loadfile)
      encoder = model[1]
      decoder = model[2]
      opt = model[3]
   end

   if opt.gpu > 0 then
      encoder:cuda()
      decoder:cuda()
      criterion:cuda()
   end

   -- Prediction
   predict(test_data, encoder, decoder)
end

main()
